{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f5daca-dd53-4511-a4db-883dbc8d25e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bb01fc-ab91-4fe6-b8b7-207070923785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K is for latent dim\n",
    "K = 256\n",
    "num_latent = 12\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, K=K, num_latent=num_latent):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.x_dim = x_dim\n",
    "        self.h_dim_1 = 256\n",
    "        self.h_dim_2 = 256\n",
    "        self.z_dim = z_dim\n",
    "        self.nc = nc\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(self.x_dim, self.h_dim_1, bias=True),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.h_dim_1, self.h_dim_2, bias=True),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.h_dim_2, self.z_dim * 2, bias=True),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.z_dim, self.h_dim_2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.h_dim_2, self.h_dim_1, bias=True),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.h_dim_1, self.x_dim, bias=True),\n",
    "        )\n",
    "        self.weight_init()\n",
    "        \n",
    "        self.fc1 = nn.Linear(784, 1024)\n",
    "        self.fc2 = nn.Linear(1024,1024)\n",
    "        self.enc_mean = nn.Linear(1024, K)\n",
    "        self.enc_std = nn.Linear(1024, K)\n",
    "        self.dec = nn.Linear(K, 10)\n",
    "        self.num_latent = num_latent\n",
    "        \n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.constant_(self.fc1.bias, 0.0)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.constant_(self.fc2.bias, 0.0)\n",
    "        nn.init.xavier_uniform_(self.enc_mean.weight)\n",
    "        nn.init.constant_(self.enc_mean.bias, 0.0)\n",
    "        nn.init.xavier_uniform_(self.enc_std.weight)\n",
    "        nn.init.constant_(self.enc_std.bias, 0.0)\n",
    "        nn.init.xavier_uniform_(self.dec.weight)\n",
    "        nn.init.constant_(self.dec.bias, 0.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = 2*x - 1. # input normalize to [-1,1]\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        enc_mean, enc_std = self.enc_mean(x), F.softplus(self.enc_std(x)-5)\n",
    "        latent = enc_mean.unsqueeze(1) + torch.randn((tuple(enc_std.shape)[0],)\\\n",
    "                                        +(self.num_latent,)\\\n",
    "                                        + tuple(enc_std.shape)[1:]).cuda() * enc_std.unsqueeze(1)\n",
    "        outputs = F.softmax(self.dec(latent),dim=2)\n",
    "        output = torch.mean(outputs, dim=1)\n",
    "        return outputs, output, enc_mean, enc_std\n",
    "    \n",
    "def update_target(target, original, update_rate):\n",
    "        for target_param, param in zip(target.parameters(), original.parameters()):\n",
    "            target_param.data.copy_((1.0 - update_rate) * target_param.data + update_rate*param.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
